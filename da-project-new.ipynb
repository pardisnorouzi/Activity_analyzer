{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/pardisnorouzi/da-project-new?scriptVersionId=122487671\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nimport numpy as np\nfrom sklearn.svm import SVC\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom scipy import stats\nfrom sklearn.model_selection import GridSearchCV\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import precision_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('/'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-17T15:50:15.46343Z","iopub.execute_input":"2023-03-17T15:50:15.463852Z","iopub.status.idle":"2023-03-17T15:50:16.950674Z","shell.execute_reply.started":"2023-03-17T15:50:15.463818Z","shell.execute_reply":"2023-03-17T15:50:16.949635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/mobile-health/mhealth_raw_data.csv\")\ndf","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:50:16.952474Z","iopub.execute_input":"2023-03-17T15:50:16.952793Z","iopub.status.idle":"2023-03-17T15:50:22.047399Z","shell.execute_reply.started":"2023-03-17T15:50:16.952764Z","shell.execute_reply":"2023-03-17T15:50:22.046073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# count null values in each column\nnull_counts = df.isnull().sum()\n\n# print the null counts\nprint(null_counts)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:50:22.048986Z","iopub.execute_input":"2023-03-17T15:50:22.049389Z","iopub.status.idle":"2023-03-17T15:50:22.149304Z","shell.execute_reply.started":"2023-03-17T15:50:22.049353Z","shell.execute_reply":"2023-03-17T15:50:22.14845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df == 0].count().sum()","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:50:22.151594Z","iopub.execute_input":"2023-03-17T15:50:22.152548Z","iopub.status.idle":"2023-03-17T15:50:22.549644Z","shell.execute_reply.started":"2023-03-17T15:50:22.152511Z","shell.execute_reply":"2023-03-17T15:50:22.548544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop(['subject'], axis=1)\ndf = df[df['Activity']>0]\ndf","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:50:22.551217Z","iopub.execute_input":"2023-03-17T15:50:22.551641Z","iopub.status.idle":"2023-03-17T15:50:22.66551Z","shell.execute_reply.started":"2023-03-17T15:50:22.551597Z","shell.execute_reply":"2023-03-17T15:50:22.664149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:50:22.666833Z","iopub.execute_input":"2023-03-17T15:50:22.667121Z","iopub.status.idle":"2023-03-17T15:50:22.693438Z","shell.execute_reply.started":"2023-03-17T15:50:22.667093Z","shell.execute_reply":"2023-03-17T15:50:22.692215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the current memory usage of the DataFrame\nprint(f\"Memory usage before downcasting: {df.memory_usage(deep=True).sum() / 1024 ** 2:.2f} MB\")\n\n# Downcast the DataFrame as you have done before\ndf_downcasted = df.copy()\n\n# Loop through each column in the DataFrame\nfor col in df_downcasted.columns:\n    # Downcast float64 columns to float16\n    if df_downcasted[col].dtype == 'float64':\n        df_downcasted[col] = df_downcasted[col].astype('float16')\n\n# Get the memory usage of the DataFrame after downcasting\nprint(f\"Memory usage after downcasting: {df_downcasted.memory_usage(deep=True).sum() / 1024 ** 2:.2f} MB\")","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:50:22.694786Z","iopub.execute_input":"2023-03-17T15:50:22.695098Z","iopub.status.idle":"2023-03-17T15:50:22.805971Z","shell.execute_reply.started":"2023-03-17T15:50:22.695069Z","shell.execute_reply":"2023-03-17T15:50:22.804914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Select the columns to plot\ncols_to_plot = [\"alx\", \"aly\", \"alz\", \"glx\", \"gly\", \"glz\", \"arx\", \"ary\", \"arz\", \"grx\", \"gry\", \"grz\"]\n\n# Create a boxplot showing the data for each feature\nfig, ax = plt.subplots(figsize=(10, 8))\nax = sns.boxplot(data=df[cols_to_plot], palette='coolwarm')\nax.set_title('Boxplot of Dataset (Excluding Activity Column)')\nax.set_ylabel('Value')\nax.set_xlabel('Feature')\nplt.xticks(rotation=45, ha='right')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:50:22.807443Z","iopub.execute_input":"2023-03-17T15:50:22.80776Z","iopub.status.idle":"2023-03-17T15:50:23.83427Z","shell.execute_reply.started":"2023-03-17T15:50:22.807732Z","shell.execute_reply":"2023-03-17T15:50:23.832938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the list of activities\nactivities = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\"]\n\n# Define the number of rows and columns for the subplots\nnum_rows = 4\nnum_cols = 3\n\n# Create the subplots\nfig, axs = plt.subplots(num_rows, num_cols, figsize=(18, 18))\naxs = axs.ravel()\n\n# Iterate over each activity\nfor i, act in enumerate(activities):\n    # Filter the dataset by the current activity\n    df_temp = df.loc[df.Activity == int(act)].copy()\n    df_temp.reset_index(inplace=True)\n\n    # Convert the time to seconds and start with 0\n    df_temp['time_sec'] = df_temp.index/50\n\n    # Plot the accelerometer readings on the current subplot\n    axs[i].plot(df_temp['time_sec'], df_temp['alx'], color='red', alpha=0.5, label='alx')\n    axs[i].plot(df_temp['time_sec'], df_temp['aly'], color='green', alpha=0.5, label='aly')\n    axs[i].plot(df_temp['time_sec'], df_temp['alz'], color='blue', alpha=0.5, label='alz')\n    axs[i].set_ylim(-20, 20)\n    axs[i].set_title('Activity ' + act + ' - Accelerometer Readings (Left Arm)')\n    axs[i].legend()\n    axs[i].grid()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:50:23.835847Z","iopub.execute_input":"2023-03-17T15:50:23.837041Z","iopub.status.idle":"2023-03-17T15:50:30.24341Z","shell.execute_reply.started":"2023-03-17T15:50:23.83699Z","shell.execute_reply":"2023-03-17T15:50:30.242405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_style('whitegrid') \n\nsns.countplot(x='Activity', data=df_downcasted) \n\n\nplt.title('Number of samples by activity') \n\nplt.show() ","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:50:30.247209Z","iopub.execute_input":"2023-03-17T15:50:30.247821Z","iopub.status.idle":"2023-03-17T15:50:30.578815Z","shell.execute_reply.started":"2023-03-17T15:50:30.247785Z","shell.execute_reply":"2023-03-17T15:50:30.577284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"window = 1                 # window dimention in seconds \nfs = 50                    # frequency in Hz\nwindows_size = window * fs # number of samples for each time window","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:50:30.580363Z","iopub.execute_input":"2023-03-17T15:50:30.580786Z","iopub.status.idle":"2023-03-17T15:50:30.586062Z","shell.execute_reply.started":"2023-03-17T15:50:30.580745Z","shell.execute_reply":"2023-03-17T15:50:30.584902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tsfel","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:50:30.588309Z","iopub.execute_input":"2023-03-17T15:50:30.58921Z","iopub.status.idle":"2023-03-17T15:50:46.895746Z","shell.execute_reply.started":"2023-03-17T15:50:30.589145Z","shell.execute_reply":"2023-03-17T15:50:46.894339Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tsfel\nimport time","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:50:46.897697Z","iopub.execute_input":"2023-03-17T15:50:46.898053Z","iopub.status.idle":"2023-03-17T15:50:47.318285Z","shell.execute_reply.started":"2023-03-17T15:50:46.898019Z","shell.execute_reply":"2023-03-17T15:50:47.316918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FEATURES = True\nSAVE_FEATURES = True","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:50:47.320132Z","iopub.execute_input":"2023-03-17T15:50:47.320667Z","iopub.status.idle":"2023-03-17T15:50:47.327353Z","shell.execute_reply.started":"2023-03-17T15:50:47.32062Z","shell.execute_reply":"2023-03-17T15:50:47.326226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if FEATURES:\n    # Python3 program to Convert a list to dictionary \n    def Convert(lst):\n        res_dct = {lst[i]: i for i in range(0, len(lst))}\n        return res_dct\n\n    # Driver code\n    res_dct = Convert(df['Activity'].unique())\n    res_dct ","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:50:47.328794Z","iopub.execute_input":"2023-03-17T15:50:47.329237Z","iopub.status.idle":"2023-03-17T15:50:47.341869Z","shell.execute_reply.started":"2023-03-17T15:50:47.329194Z","shell.execute_reply":"2023-03-17T15:50:47.340845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if FEATURES:\n    df.Activity = df['Activity'].map(res_dct)\n    del res_dct\n    display(df['Activity'].unique())\n    display(df.head())","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:50:47.343819Z","iopub.execute_input":"2023-03-17T15:50:47.344267Z","iopub.status.idle":"2023-03-17T15:50:47.384687Z","shell.execute_reply.started":"2023-03-17T15:50:47.344231Z","shell.execute_reply":"2023-03-17T15:50:47.383625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if FEATURES:\n    t1 = time.time()\n    # Retrieves a pre-defined feature configuration file to extract all available features\n    # Available domains: \"statistical\"; \"spectral\"; \"temporal\"\n    cfg_file = tsfel.get_features_by_domain()\n\n    # Extract the Activity classes for each created window \n    tmp_class = df_downcasted['Activity'].values\n    tmp_class = tmp_class[0:int(len(tmp_class)/windows_size)*windows_size].reshape((int(len(tmp_class)/windows_size),windows_size))\n    tmp_class = pd.DataFrame(tmp_class).mode(axis=1)\n\n    # Extract features\n    tmp_features = tsfel.time_series_features_extractor(cfg_file, df_downcasted.iloc[0:,0:12], fs = fs, window_size=windows_size, header_names = df_downcasted.columns[0:12].values, n_jobs = -1)\n\n    tmp_features['Activity'] = tmp_class\n    df_features = tmp_features\n    t2 = time.time()\n    print('Elapsed time [s]:', np.round(t2-t1,4))\n\n    del tmp_class, t1, t2, tmp_features\n\n    display(df_features.head(5))\n    display(df_features.shape)\n    df_downcasted = df_features.copy()\n","metadata":{"execution":{"iopub.status.busy":"2023-03-17T15:50:47.386558Z","iopub.execute_input":"2023-03-17T15:50:47.387007Z","iopub.status.idle":"2023-03-17T16:15:55.08081Z","shell.execute_reply.started":"2023-03-17T15:50:47.386958Z","shell.execute_reply":"2023-03-17T16:15:55.07953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_downcasted.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-03-17T16:15:55.082689Z","iopub.execute_input":"2023-03-17T16:15:55.083905Z","iopub.status.idle":"2023-03-17T16:15:55.127884Z","shell.execute_reply.started":"2023-03-17T16:15:55.083856Z","shell.execute_reply":"2023-03-17T16:15:55.126831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_downcasted.info()","metadata":{"execution":{"iopub.status.busy":"2023-03-17T16:15:55.129508Z","iopub.execute_input":"2023-03-17T16:15:55.130408Z","iopub.status.idle":"2023-03-17T16:15:55.221148Z","shell.execute_reply.started":"2023-03-17T16:15:55.130361Z","shell.execute_reply":"2023-03-17T16:15:55.220304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_downcasted.head())","metadata":{"execution":{"iopub.status.busy":"2023-03-17T16:15:55.222264Z","iopub.execute_input":"2023-03-17T16:15:55.223003Z","iopub.status.idle":"2023-03-17T16:15:55.241093Z","shell.execute_reply.started":"2023-03-17T16:15:55.222966Z","shell.execute_reply":"2023-03-17T16:15:55.239921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Calculate the correlation matrix\ncorr_matrix = df_downcasted.corr()\n\n# Create a larger figure\nplt.figure(figsize=(14,12))\n\n# Create a heatmap of the correlation matrix\nsns.heatmap(corr_matrix, cmap='coolwarm')\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-17T16:15:55.242592Z","iopub.execute_input":"2023-03-17T16:15:55.242967Z","iopub.status.idle":"2023-03-17T16:17:08.832912Z","shell.execute_reply.started":"2023-03-17T16:15:55.242933Z","shell.execute_reply":"2023-03-17T16:17:08.831399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"to_drop = set()\ncorr_matrix = df_features.corr().abs()\nupper = np.triu(corr_matrix, k=1)\nupper_df = pd.DataFrame(upper, columns=corr_matrix.columns, index=corr_matrix.index)\nfor col in upper_df.columns:\n    if any(upper_df[col] > 0.95):\n        to_drop.add(col)\n\ndf_downcasted = df_downcasted.drop(columns=to_drop)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-17T16:17:08.835084Z","iopub.execute_input":"2023-03-17T16:17:08.835688Z","iopub.status.idle":"2023-03-17T16:18:15.480572Z","shell.execute_reply.started":"2023-03-17T16:17:08.835631Z","shell.execute_reply":"2023-03-17T16:18:15.479488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_downcasted.columns","metadata":{"execution":{"iopub.status.busy":"2023-03-17T16:18:15.482107Z","iopub.execute_input":"2023-03-17T16:18:15.482573Z","iopub.status.idle":"2023-03-17T16:18:15.490598Z","shell.execute_reply.started":"2023-03-17T16:18:15.482518Z","shell.execute_reply":"2023-03-17T16:18:15.48911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_downcasted)\ndf_downcasted.info()\n","metadata":{"execution":{"iopub.status.busy":"2023-03-17T16:18:15.492153Z","iopub.execute_input":"2023-03-17T16:18:15.492848Z","iopub.status.idle":"2023-03-17T16:18:15.569579Z","shell.execute_reply.started":"2023-03-17T16:18:15.492806Z","shell.execute_reply":"2023-03-17T16:18:15.568364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = df_downcasted.drop('Activity', axis=1)\ny = df_downcasted['Activity']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nprint(\"Train set shape before scaling:\", X_train.shape)\nprint(\"Test set shape before scaling:\", X_test.shape)\n\n# Create a StandardScaler object\nscaler = StandardScaler()\n\n# Fit the scaler to the training data and transform it\nX_train_scaled = scaler.fit_transform(X_train)\n\n# Transform the test data using the fitted scaler\nX_test_scaled = scaler.transform(X_test)\n\nprint(\"Train set shape after scaling:\", X_train_scaled.shape)\nprint(\"Test set shape after scaling:\", X_test_scaled.shape)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-03-17T16:18:15.571323Z","iopub.execute_input":"2023-03-17T16:18:15.572448Z","iopub.status.idle":"2023-03-17T16:18:15.786378Z","shell.execute_reply.started":"2023-03-17T16:18:15.572406Z","shell.execute_reply":"2023-03-17T16:18:15.784831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n# assuming y is your target variable after scaling\nunique, counts = np.unique(y, return_counts=True)\nclass_counts = dict(zip(unique, counts))\nprint(\"Class counts:\", class_counts)\n","metadata":{"execution":{"iopub.status.busy":"2023-03-17T16:18:15.787931Z","iopub.execute_input":"2023-03-17T16:18:15.788739Z","iopub.status.idle":"2023-03-17T16:18:15.795735Z","shell.execute_reply.started":"2023-03-17T16:18:15.788705Z","shell.execute_reply":"2023-03-17T16:18:15.794604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the parameter grid for the Random Forest Classifier\nparam_grid = {\n    'n_estimators': [50, 100, 150],\n    'max_depth': [10, 20, None],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'max_features': ['auto', 'sqrt']\n}\n\n# Create a Random Forest Classifier\nrf = RandomForestClassifier(random_state=42)\n\n# Create a GridSearchCV object and fit it to the training data\ngrid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1)\ngrid_search.fit(X_train_scaled, y_train)\n\n# Print the best parameters and best score\nprint(\"Best parameters: \", grid_search.best_params_)\nprint(\"Best score: \", grid_search.best_score_)\n\n# Get the best estimator from grid search and predict on test data\nbest_rf = grid_search.best_estimator_\ny_pred = best_rf.predict(X_test_scaled)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T16:18:15.797707Z","iopub.execute_input":"2023-03-17T16:18:15.798189Z","iopub.status.idle":"2023-03-17T16:19:28.984952Z","shell.execute_reply.started":"2023-03-17T16:18:15.798114Z","shell.execute_reply":"2023-03-17T16:19:28.981399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model's performance\ncm = confusion_matrix(y_test, y_pred)\nspecificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\nsensitivity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\nprecision = cm[1, 1] / (cm[0, 1] + cm[1, 1])\nf1 = 2 * (precision * sensitivity) / (precision + sensitivity)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(\"Confusion matrix:\\n\", cm)\nprint(\"Specificity:\", specificity)\nprint(\"Sensitivity:\", sensitivity)\nprint(\"Precision:\", precision)\nprint(\"F1 score:\", f1)\nprint(\"Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T16:19:28.986549Z","iopub.status.idle":"2023-03-17T16:19:28.987902Z","shell.execute_reply.started":"2023-03-17T16:19:28.987659Z","shell.execute_reply":"2023-03-17T16:19:28.987686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\nX = df_downcasted.drop('Activity', axis=1)\ny = df_downcasted['Activity']\n\nX_train_scaled, X_test_scaled, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # change test_size to 0.2\n\n# Define the parameter grid for the Random Forest Classifier\nparam_grid = {\n    'n_estimators': [50, 100, 150],\n    'max_depth': [10, 20, None],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'max_features': ['auto', 'sqrt']\n}\n\n# Create a Random Forest Classifier\nrf = RandomForestClassifier(random_state=42)\n\n# Create a GridSearchCV object and fit it to the training data\ngrid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1)\ngrid_search.fit(X_train_scaled, y_train)\n\n# Print the best parameters and best score\nprint(\"Best parameters: \", grid_search.best_params_)\nprint(\"Best score: \", grid_search.best_score_)\n\n# Get the best estimator from grid search and predict on test data\nbest_rf = grid_search.best_estimator_\ny_pred = best_rf.predict(X_test_scaled)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T16:19:28.989331Z","iopub.status.idle":"2023-03-17T16:19:28.989748Z","shell.execute_reply.started":"2023-03-17T16:19:28.989548Z","shell.execute_reply":"2023-03-17T16:19:28.98957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model's performance\ncm = confusion_matrix(y_test, y_pred)\nspecificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\nsensitivity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\nprecision = cm[1, 1] / (cm[0, 1] + cm[1, 1])\nf1 = 2 * (precision * sensitivity) / (precision + sensitivity)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(\"Confusion matrix:\\n\", cm)\nprint(\"Specificity:\", specificity)\nprint(\"Sensitivity:\", sensitivity)\nprint(\"Precision:\", precision)\nprint(\"F1 score:\", f1)\nprint(\"Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T16:19:28.991544Z","iopub.status.idle":"2023-03-17T16:19:28.992561Z","shell.execute_reply.started":"2023-03-17T16:19:28.992354Z","shell.execute_reply":"2023-03-17T16:19:28.992377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#SVM\n# Define the parameter grid for the SVM\nparam_grid = {\n    'C': [0.1, 1, 10],\n    'kernel': ['linear', 'rbf', 'sigmoid'],\n    'gamma': ['scale', 'auto']\n}\n\n# Create an SVM classifier\nsvm = SVC(random_state=42)\n\n# Create a GridSearchCV object and fit it to the training data\ngrid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5, n_jobs=-1)\ngrid_search.fit(X_train_scaled, y_train)\n\n# Print the best parameters and best score\nprint(\"Best parameters: \", grid_search.best_params_)\nprint(\"Best score: \", grid_search.best_score_)\n\n# Get the best estimator from grid search and predict on test data\nbest_rf = grid_search.best_estimator_\ny_pred = best_rf.predict(X_test_scaled)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T16:19:28.993639Z","iopub.status.idle":"2023-03-17T16:19:28.994053Z","shell.execute_reply.started":"2023-03-17T16:19:28.993823Z","shell.execute_reply":"2023-03-17T16:19:28.993861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model's performance\ncm = confusion_matrix(y_test, y_pred)\nspecificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\nsensitivity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\nprecision = cm[1, 1] / (cm[0, 1] + cm[1, 1])\nf1 = 2 * (precision * sensitivity) / (precision + sensitivity)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(\"Confusion matrix:\\n\", cm)\nprint(\"Specificity:\", specificity)\nprint(\"Sensitivity:\", sensitivity)\nprint(\"Precision:\", precision)\nprint(\"F1 score:\", f1)\nprint(\"Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T16:19:28.995248Z","iopub.status.idle":"2023-03-17T16:19:28.995605Z","shell.execute_reply.started":"2023-03-17T16:19:28.995436Z","shell.execute_reply":"2023-03-17T16:19:28.995453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Logistic Regression\n# Define the parameter grid for the Logistic Regression Classifier\nparam_grid = {\n    'C': [0.1, 1, 10],\n    'penalty': ['l1', 'l2'],\n    'solver': ['liblinear', 'saga']\n}\n\n# Create a Logistic Regression Classifier\nlr = LogisticRegression(random_state=42)\n\n# Create a GridSearchCV object and fit it to the training data\ngrid_search = GridSearchCV(estimator=lr, param_grid=param_grid, cv=5, n_jobs=-1)\ngrid_search.fit(X_train_scaled, y_train)\n\n# Print the best parameters and best score\nprint(\"Best parameters: \", grid_search.best_params_)\nprint(\"Best score: \", grid_search.best_score_)\n\n# Get the best estimator from grid search and predict on test data\nbest_rf = grid_search.best_estimator_\ny_pred = best_rf.predict(X_test_scaled)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T16:19:28.997822Z","iopub.status.idle":"2023-03-17T16:19:28.998521Z","shell.execute_reply.started":"2023-03-17T16:19:28.998215Z","shell.execute_reply":"2023-03-17T16:19:28.998247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model's performance\ncm = confusion_matrix(y_test, y_pred)\nspecificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\nsensitivity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\nprecision = cm[1, 1] / (cm[0, 1] + cm[1, 1])\nf1 = 2 * (precision * sensitivity) / (precision + sensitivity)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(\"Confusion matrix:\\n\", cm)\nprint(\"Specificity:\", specificity)\nprint(\"Sensitivity:\", sensitivity)\nprint(\"Precision:\", precision)\nprint(\"F1 score:\", f1)\nprint(\"Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T16:19:28.999894Z","iopub.status.idle":"2023-03-17T16:19:29.000474Z","shell.execute_reply.started":"2023-03-17T16:19:29.000165Z","shell.execute_reply":"2023-03-17T16:19:29.00022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Decision Tree\nfrom sklearn.exceptions import ConvergenceWarning\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n\n# Define the parameter grid for the Decision Tree Classifier\nparam_grid = {\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [10, 20, None],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'max_features': ['auto', 'sqrt']\n}\n\n# Create a Decision Tree Classifier\ndtc = DecisionTreeClassifier(random_state=42)\n\n# Create a GridSearchCV object and fit it to the training data\ngrid_search = GridSearchCV(estimator=dtc, param_grid=param_grid, cv=5, n_jobs=-1)\ngrid_search.fit(X_train_scaled, y_train)\n\n# Print the best parameters and best score\nprint(\"Best parameters: \", grid_search.best_params_)\nprint(\"Best score: \", grid_search.best_score_)\n\n# Get the best estimator from grid search and predict on test data\nbest_rf = grid_search.best_estimator_\ny_pred = best_rf.predict(X_test_scaled)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T16:19:29.002922Z","iopub.status.idle":"2023-03-17T16:19:29.003792Z","shell.execute_reply.started":"2023-03-17T16:19:29.003474Z","shell.execute_reply":"2023-03-17T16:19:29.003506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model's performance\ncm = confusion_matrix(y_test, y_pred)\nspecificity = cm[0, 0] / (cm[0, 0] + cm[0, 1])\nsensitivity = cm[1, 1] / (cm[1, 0] + cm[1, 1])\nprecision = cm[1, 1] / (cm[0, 1] + cm[1, 1])\nf1 = 2 * (precision * sensitivity) / (precision + sensitivity)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(\"Confusion matrix:\\n\", cm)\nprint(\"Specificity:\", specificity)\nprint(\"Sensitivity:\", sensitivity)\nprint(\"Precision:\", precision)\nprint(\"F1 score:\", f1)\nprint(\"Accuracy:\", accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-03-17T16:19:29.004932Z","iopub.status.idle":"2023-03-17T16:19:29.005364Z","shell.execute_reply.started":"2023-03-17T16:19:29.005113Z","shell.execute_reply":"2023-03-17T16:19:29.005132Z"},"trusted":true},"execution_count":null,"outputs":[]}]}